# GANs in PyTorch
In this repository I implement several versions of Generative Adversarial Networks in PyTorch.

All comments and discussions are welcome.

# GAN
In this section I implemented the original version of GAN as described in [this paper](https://arxiv.org/abs/1406.2661) by Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio

## Experiment setup
The target distribution was a Normal distribution with mean=3 and std=1, and the input noise to the generator was sampled from a uniform distribution. Both the target and noise samples are monodimensional, but this can be changed in the config.yml file in order to extend to multiple dimensions.

The discriminator is composed by 3 hidden layers with 16, 16 and 8 neurons respectively, with ReLU activation functions and dropout after each layer with a probability of 0.5. The output layer is composed by only 1 neuron with sigmoid activation function, providing the probability of the input sample belonging to the real distribution and not being generated by the generator.

The generator is composed by 3 hidden layers of sizes 16, 32, 16 relatively, with ReLU activation functions. The output layer has the same size of the number of dimensions of the target samples, so in our case is 1. The output activation function is linear, because we don't want to limit the output values.

## Results
Here some results are reported after training for 20k steps.

The training of the two networks was quite unstable and with high variance, where different runs with the same setup but different initializations produced very different results in some cases. A possible next step could be to reduce the size of the networks (reducing the number of neurons per layer or the number of layers) in order to obtain more stable models.

Below are reported some charts of the results of the experiment. As you can see, the generator matched the range and the mean of the real data distribution, even if it's not perfectly getting the good bell shape.<br><br>

<p align="center"><img src="GAN/19-02-27_10-11/generated_vs_real_distribution.png" alt="Distributions" width="500" height="400"></p>

Discriminator Loss                                           |  Generator Loss
:-----------------------------------------------------------:|:---------------------------------------:
![Discriminator Loss](GAN/19-02-27_10-11/discriminator_loss.png) | ![Generator Loss](GAN/19-02-27_10-11/generator_loss.png)

As we can see, the generator was able to catch the real mean of the data after around 15k steps.<br><br>

<p align="center"><img src="GAN/19-02-27_10-11/mean.png" alt="Mean" width="500" height="400"></p>

# Deep Convolutional GAN (DCGAN) - Work in Progress
This will be the implementation of GAN using Deep Convolutional Neural Networks as described in [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434) by Alec Radford, Luke Metz and Soumith Chintala.
